{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import RegexpParser\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import Tree\n",
    "\n",
    "import PyPDF2 \n",
    "import textract\n",
    "import re\n",
    "\n",
    "from os import path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, SpatialDropout1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from functools import reduce\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model   \n",
    "from keras.preprocessing.text import one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from word2vec Corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading corpus and creating vectorized data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def word_vectors_list():\n",
    "    \n",
    "    vector_model = Word2Vec.load('ankit_word2vec_model.bin')\n",
    "    dataset = list(vector_model.wv.vocab)\n",
    "\n",
    "    data_vector_list = []\n",
    "\n",
    "    for i in range(0, len(dataset)):\n",
    "        try:\n",
    "            data  = [[str(dataset[i])]]\n",
    "\n",
    "            model = Word2Vec(data, min_count = 1, size=100)\n",
    "            data_word  = model[dataset[i]] \n",
    "\n",
    "            data_vector_list = data_vector_list + [data_word]\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return(data_vector_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_list():\n",
    "    \n",
    "    vector_model = Word2Vec.load('ankit_word2vec_model.bin')\n",
    "    data_list = list(vector_model.wv.vocab)\n",
    "        \n",
    "    return(data_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram generation.\n",
    "#### Function for generating n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_ngrams(s, n):\n",
    "    \n",
    "    # Convert to lowercases.\n",
    "    #--------------------------------------------------------\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces.\n",
    "    #--------------------------------------------------------\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s+-]', '', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens.\n",
    "    #--------------------------------------------------------\n",
    "    token = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return.\n",
    "    #--------------------------------------------------------\n",
    "    ngrams = zip(*[token[i:] for i in range(n)])\n",
    "    \n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of stop word and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # custom stopword list.\n",
    "    #----------------------------\n",
    "    stop_word_data = pd.read_csv('stopwords/sw3.txt')\n",
    "    stop_word_data = list(stop_word_data['stopwords'])\n",
    "    \n",
    "    # filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    filtered_words = [w for w in tokens if not w in stop_word_data]\n",
    "    \n",
    "    return \" \".join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating processed data.\n",
    "#### removing stop word and creating n_grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_formation(data):\n",
    "    \n",
    "    text_data = []\n",
    "    \n",
    "    data = preprocess_text(data)\n",
    "\n",
    "    uni_gram_data = generate_ngrams(data, 1)\n",
    "    bi_gram_data  = generate_ngrams(data, 2)\n",
    "    tri_gram_data = generate_ngrams(data, 3)\n",
    "    \n",
    "    text_data = uni_gram_data + bi_gram_data + tri_gram_data\n",
    "    \n",
    "    text_data = list(dict.fromkeys(text_data))\n",
    "    \n",
    "    return(text_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting pdf to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_from_pdf(file_loc):\n",
    "   \n",
    "    filename = file_loc\n",
    "\n",
    "    # open allows you to read the file.\n",
    "    #------------------------------------\n",
    "    pdfFileObj = open(filename,'rb')\n",
    "\n",
    "    # The pdfReader variable is a readable object that will be parsed.\n",
    "    #-----------------------------------------------------------------\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "    # Discerning the number of pages will allow us to parse through all #the pages.\n",
    "    #-----------------------------------------------------------------------------\n",
    "    num_pages = pdfReader.numPages\n",
    "    count = 0\n",
    "    text = \"\"\n",
    "\n",
    "    # The while loop will read each page.\n",
    "    #------------------------------------\n",
    "    while count < num_pages:\n",
    "        pageObj = pdfReader.getPage(count)\n",
    "        count +=1\n",
    "        text += pageObj.extractText()\n",
    "\n",
    "    # This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned files.\n",
    "    #----------------------------------------------------------------------------------------------------------------------------\n",
    "    if text != \"\":\n",
    "        text = text\n",
    "\n",
    "    #If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    else:\n",
    "        text = textract.process(filename, method='tesseract', language='eng')\n",
    " \n",
    "    return(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding.\n",
    "#### Finding emmbed_dimension, pad_sequence, vocab_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encoded_data(model_data) :\n",
    "        \n",
    "    # emmbedding dimension.\n",
    "    #-----------------------\n",
    "    EMBEDDING_DIM = 100\n",
    "    print('embedding dimension: ', EMBEDDING_DIM)\n",
    "    \n",
    "    # pad sequence { total number of words in a single phrase }.\n",
    "    #----------------------------------------------------------------\n",
    "    max_length = max([len(s.split()) for s in model_data])\n",
    "    print('pad sequence :', max_length)\n",
    "\n",
    "    # define vocabulary size\n",
    "    #----------------------------------------------------------------\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(model_data)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print('vocabulary size :', vocab_size)\n",
    "\n",
    "    # encoding and pad documents to a max length of 4 words'.\n",
    "    #---------------------------------------------------------\n",
    "    encoded_docs = [one_hot(d, vocab_size) for d in model_data]\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    \n",
    "    return([EMBEDDING_DIM, max_length, vocab_size, padded_docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vectorization(word_list, sav_non_tech):    \n",
    "    \n",
    "    vec_word = []\n",
    "    model = Word2Vec.load('ankit_word2vec_model.bin')\n",
    "    word_list = data_formation(word_list) \n",
    "        \n",
    "    for word in range(0, len(word_list)):\n",
    "        \n",
    "        try:\n",
    "            vec_tech_word = model.wv.most_similar(positive=[word_list[word]])\n",
    "            vec_word = vec_word + [word_list[word]]\n",
    "        except:\n",
    "            if(sav_non_tech == True):\n",
    "                saving_non_tech_word(word_list[word])\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    if(len(vec_word)==0):\n",
    "        return('no technical word found.')\n",
    "    else:        \n",
    "        return(vec_word)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving parameters.\n",
    "#### Saving max-length and vocab size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def saving_parameters(max_length, vocab_size):\n",
    "    \n",
    "    if(path.exists('dataset/param_data.csv') == True):\n",
    "        with open('dataset/param_data.csv', 'a') as csvFile:\n",
    "            csvFile.write('\\n{},{}'.format(max_length, vocab_size))        \n",
    "        \n",
    "    else:\n",
    "        with open('dataset/param_data.csv', 'w') as csvFile:\n",
    "            csvFile.write('{},{}'.format('max_length','vocab_size'))\n",
    "            csvFile.write('\\n{},{}'.format(max_length, vocab_size))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learninig model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Model( vocab_size, EMBEDDING_DIM, max_length):\n",
    "   \n",
    "    # Now defining our model.\n",
    "    #-------------------------\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length, trainable=True))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compiling our model.\n",
    "    #-----------------------\n",
    "    adam_opt = keras.optimizers.Adam(lr=0.0001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer = adam_opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training():\n",
    "    \n",
    "    # loading data from corpus.\n",
    "    #------------------------------------------------------------------\n",
    "    model_data_val = word_list()\n",
    "    model_data_labels = [1]*len(model_data_val)\n",
    "    model_data_1 = pd.DataFrame(list(zip(model_data_val, model_data_labels)), columns =['Phrases', 'Labels'])\n",
    "    model_data_2 = pd.read_csv('dataset/non_tech_data/train_non_tech_data.csv')\n",
    "\n",
    "    model_data = model_data_1.append(model_data_2, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # model history.\n",
    "    #-----------------\n",
    "    model_history = []\n",
    "    \n",
    "    # getting encoded data info.\n",
    "    #----------------------------------\n",
    "    encode_data = encoded_data(model_data.Phrases)\n",
    "    \n",
    "    EMBEDDING_DIM = encode_data[0]    \n",
    "    max_length = encode_data[1]\n",
    "    vocab_size = encode_data[2]\n",
    "    padded_docs = encode_data[3]\n",
    "    \n",
    "    model = Model(vocab_size, EMBEDDING_DIM, max_length)\n",
    "    model.summary()\n",
    "    \n",
    "    # fitting data to model.\n",
    "    #-----------------------------------------------------------------------\n",
    "    model.fit(padded_docs, model_data.Labels, epochs=10, batch_size = 256, verbose=2) \n",
    "    \n",
    "    # saving trained model.\n",
    "    #------------------------------------------------------------------------\n",
    "    model.save('tech_model.h5')\n",
    "    \n",
    "    print(\"\\nModel trained and saved successfully.\")\n",
    "       \n",
    "    # saving max-length and vocab-size for prediction.\n",
    "    #------------------------------------------------\n",
    "    saving_parameters(max_length, vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dimension:  100\n",
      "pad sequence : 11\n",
      "vocabulary size : 20887\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 11, 100)           2088700   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 11, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,497,533\n",
      "Trainable params: 2,497,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.5334 - acc: 0.6985\n",
      "Epoch 2/10\n",
      " - 6s - loss: 0.2957 - acc: 0.8773\n",
      "Epoch 3/10\n",
      " - 6s - loss: 0.2426 - acc: 0.9007\n",
      "Epoch 4/10\n",
      " - 6s - loss: 0.2200 - acc: 0.9108\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.2066 - acc: 0.9165\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.1987 - acc: 0.9189\n",
      "Epoch 7/10\n",
      " - 6s - loss: 0.1931 - acc: 0.9205\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.1885 - acc: 0.9227\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.1828 - acc: 0.9248\n",
      "Epoch 10/10\n",
      " - 6s - loss: 0.1811 - acc: 0.9258\n",
      "\n",
      "Model trained and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating CSV for non-technical word.\n",
    "#### Saving the non technical word into csv with label 0.  So, that we can use this csv to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def saving_non_tech_word(word):\n",
    "        \n",
    "    if(path.exists('dataset/non_tech_data/train_non_tech_data.csv') == True):\n",
    "        with open('dataset/non_tech_data/train_non_tech_data.csv', 'a') as csvFile:\n",
    "            csvFile.write('\\n{},{}'.format(word, 0))        \n",
    "        \n",
    "    else:\n",
    "        with open('dataset/non_tech_data/train_non_tech_data.csv', 'w') as csvFile:\n",
    "            csvFile.write('{},{}'.format('Phrases','Labels'))\n",
    "            csvFile.write('\\n{},{}'.format(word , 0))\n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function calling.\n",
    "### Prediction result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating output for technical and non technical word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prediction(model_name, text_data, decision_boundary, sav_non_tech):\n",
    "    \n",
    "    # loading the saved model.\n",
    "    #---------------------------\n",
    "    model = load_model(model_name)\n",
    "    \n",
    "    # loading model parameters from csv.\n",
    "    #----------------------------------\n",
    "    try:        \n",
    "        param_data = pd.read_csv('dataset/param_data.csv')\n",
    "        param_data = param_data.tail(1)\n",
    "        max_length = int(param_data.max_length)\n",
    "        vocab_size = int(param_data.vocab_size)\n",
    "    except:\n",
    "        print('Error reading param file. please check.')\n",
    "        return\n",
    "      \n",
    "    try:\n",
    "        \n",
    "        finallist = data_formation(text_data)\n",
    "\n",
    "        # defining list.\n",
    "        #----------------\n",
    "        final_pred_tech = []\n",
    "\n",
    "        # encoding.\n",
    "        #-----------\n",
    "        encoded_docs = [one_hot(d, vocab_size) for d in finallist]\n",
    "        \n",
    "        # data processing and formation.\n",
    "        #---------------------------------------------------------------------\n",
    "        vectorization_list = vectorization(text_data, sav_non_tech)\n",
    "                \n",
    "        # pad documents to a max length of 4 words'\n",
    "        # -----------------------------------------\n",
    "        padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "  \n",
    "        for i in range(0, len(padded_docs)):\n",
    "            data = padded_docs[i]    \n",
    "           \n",
    "            get_data = model.predict([[data]])\n",
    "\n",
    "            # decision boundary is increase and decrease as required manually.\n",
    "            #--------------------------------------------------------------------------------\n",
    "            if (get_data[0][0] > decision_boundary):\n",
    "                final_pred_tech.append(finallist[i])               \n",
    "\n",
    "        final_pred_tech.extend(vectorization_list)\n",
    "        final_pred_tech = list(dict.fromkeys(final_pred_tech))\n",
    "        \n",
    "        return(final_pred_tech)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error : may be empty string.\\n\\n',e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction For pdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TECHNICAL WORD]\n",
      "\n",
      " ['transcription', 'improvement', 'downtown', 'projection', 'dodge', 'flow', 'general', 'e2e', 'water', 'encoding', 'evaluation', 'spent', 'music', 'art', 'food', 'relations', 'extraction', 'nicole', 'ibm', 'family', 'money', 'currency', 'sun 2015', 'selection search', 'engine e2e', 'water mm', 'base construction', 'capture local', 'evaluation metric', 'repair estimates', 'finding music', 'state art', 'learning', 'processing', 'microsoft', 'intelligence', 'speech', 'nips', 'ieee', 'spm', 'translation', 'models', 'base', 'reinforcement', 'nlp', 'vi', 'scientists', 'vocabulary', 'context', 'wordnet', 'classification', 'features', 'output', 'matrix', 'vector', 'project', 'edge', 'motif', 'word', 'story', 'web', 'search', 'keywords', 'space', 'extract', 'smt', 'rescue', 'step', 'history', 'cisco', 'probability', 'matrices', 'shape', 'grounding', 'lunch', 'balance', 'signing', 'live', 'eos', 'man', 'online', 'docs', 'construction', 'pattern', 'app', 'texts', 'characters', 'scalability', 'tri', 'collision', 'boundary', 'racing', 'vectors', 'training', 'paper', 'capture', 'neurons', 'max', 'clear', 'nose', 'skate', 'wholesale', 'supply', 'milk', 'testing', 'english', 'range', 'nvidia', 'matching', 'estimates', 'activations', 'codes', 'semantics', 'reconstruction', 'relativity', 'listen', 'wikipedia', 'conv', 'computation', 'grammar', 'tpr', 'turing', 'indexing', 'spring', 'jaguar', 'planet', 'architecture', 'forest', 'avg', 'sem', 'parsing', 'map', 'film', 'dvd', 'player', 'e1', 'form', 'decoding', 'f1', 'star', 'writer', 'throat', 'cancer', 'fig', 'mail', 'frame', 'https', 'google', 'deep learning', 'natural language', 'language processing', 'machine learning', 'artificial intelligence', 'knowledge representation', 'machine translation', 'knowledge base', 'question answering', 'reinforcement learning', 'neural networks', 'speech recognition', 'neural network', 'image search', 'auto body', 'body language', 'open source', 'semantic analysis', 'dvd players', 'star wars', 'costa rica', 'political science', 'lexical semantics', 'natural language processing', 'statistical machine translation', 'natural language understanding']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# select the pdf location.\n",
    "#-----------------------------------------\n",
    "try:\n",
    "    file_loc = 'testing_data/a.pdf' \n",
    "    text_data = text_from_pdf(file_loc)\n",
    "except:\n",
    "    print('error occured during conversion.')\n",
    "\n",
    "# select the model here after traininig.\n",
    "#-----------------------------------------\n",
    "model_name = 'tech_model.h5'\n",
    "\n",
    "# decision boundary {currently 80%}.\n",
    "# please adjust the decision boundary as required.\n",
    "#-----------------------------------------------\n",
    "decision_boundary = 0.90    \n",
    "    \n",
    "# saving non_tech word.\n",
    "#------------------------------------\n",
    "sav_non_tech = False\n",
    "    \n",
    "# list containing prediction value.\n",
    "#------------------------------------\n",
    "\n",
    "prediction_list = prediction(model_name, text_data, decision_boundary, sav_non_tech)\n",
    "print('[TECHNICAL WORD]\\n\\n', prediction_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction For Single String. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TECHNICAL WORD]\n",
      "\n",
      " ['finance', 'intelligence', 'reinforcement', 'learning', 'ajax', 'javascript', 'qt', 'framework', 'analytics', 'gui', 'android', 'django', 'project', 'python', 'dashboard', 'budgeting', 'neural networks', 'artificial intelligence', 'reinforcement learning', 'machine learning', 'qt framework', 'data analytics', 'big data', 'business intelligence', 'financial data', 'artificial neural networks', 'business intelligence tools']\n"
     ]
    }
   ],
   "source": [
    "# string.\n",
    "#-----------------------------------------\n",
    "text_data = \"I @ this artificial neural networks artificial intelligence, reinforcement learning used c++ machine learning artificial artificial intelligence artificial \"+\\\n",
    "           \"intelligence ajax javascript over QT Framework in order to get data data analytics and big data a nice GUI for what will be android further be an Django Project \"+\\\n",
    "           \"with Python etc. Bachelor's degree in Finance Excellent experience of handling Business Intelligence Tools and Dashboard Reports Skilled at \"+\\\n",
    "           \"consolidating and analyzing Financial Data Highly capable of Budgeting 3000 dollar, \"\n",
    "\n",
    "# select the model here after traininig.\n",
    "#-----------------------------------------\n",
    "model_name = 'tech_model.h5'\n",
    "\n",
    "# decision boundary {currently 80%}.\n",
    "# please adjust the decision boundary as required.\n",
    "#-----------------------------------------------\n",
    "decision_boundary = 0.90   \n",
    "\n",
    "# saving non_tech word.\n",
    "#------------------------------------\n",
    "sav_non_tech = False\n",
    "\n",
    "# list containing prediction value.\n",
    "#------------------------------------\n",
    "\n",
    "prediction_list = prediction(model_name, text_data, decision_boundary, sav_non_tech)\n",
    "print('[TECHNICAL WORD]\\n\\n', prediction_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
